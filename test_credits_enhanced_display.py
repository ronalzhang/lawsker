#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºç‰ˆCreditsä½™é¢æ˜¾ç¤ºå’Œæ”¯ä»˜æµç¨‹ç”¨æˆ·ç†è§£åº¦
éªŒè¯æ˜¯å¦è¾¾åˆ° >95% ç”¨æˆ·ç†è§£åº¦ç›®æ ‡
"""

import asyncio
import json
import sys
import os
from datetime import datetime, date, timedelta
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from backend.app.api.v1.endpoints.credits import (
    _get_balance_status,
    _get_reset_countdown, 
    _get_usage_summary,
    _get_user_recommendations,
    _calculate_pricing_info
)

class CreditsDisplayTest:
    """Creditsæ˜¾ç¤ºå¢å¼ºæµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.user_understanding_scores = []
        
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹æµ‹è¯•å¢å¼ºç‰ˆCreditsä½™é¢æ˜¾ç¤ºå’Œæ”¯ä»˜æµç¨‹...")
        print("=" * 60)
        
        # æµ‹è¯•ä½™é¢çŠ¶æ€æ˜¾ç¤º
        self.test_balance_status_display()
        
        # æµ‹è¯•é‡ç½®å€’è®¡æ—¶æ˜¾ç¤º
        self.test_reset_countdown_display()
        
        # æµ‹è¯•ä½¿ç”¨æƒ…å†µæ‘˜è¦
        self.test_usage_summary_display()
        
        # æµ‹è¯•ä¸ªæ€§åŒ–æ¨è
        self.test_user_recommendations()
        
        # æµ‹è¯•ä»·æ ¼è®¡ç®—å’Œæ˜¾ç¤º
        self.test_pricing_display()
        
        # æµ‹è¯•é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦
        self.test_error_message_clarity()
        
        # æµ‹è¯•æ”¯ä»˜æµç¨‹æŒ‡å¯¼
        self.test_payment_flow_guidance()
        
        # è®¡ç®—ç”¨æˆ·ç†è§£åº¦è¯„åˆ†
        self.calculate_user_understanding_score()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        return self.generate_test_report()
    
    def test_balance_status_display(self):
        """æµ‹è¯•ä½™é¢çŠ¶æ€æ˜¾ç¤ºçš„æ¸…æ™°åº¦"""
        print("ğŸ“Š æµ‹è¯•ä½™é¢çŠ¶æ€æ˜¾ç¤º...")
        
        test_cases = [
            {"credits": 0, "expected_level": "empty", "expected_action": True},
            {"credits": 1, "expected_level": "low", "expected_action": False},
            {"credits": 3, "expected_level": "normal", "expected_action": False},
            {"credits": 5, "expected_level": "good", "expected_action": False},
            {"credits": 10, "expected_level": "good", "expected_action": False}
        ]
        
        clarity_scores = []
        
        for case in test_cases:
            status = _get_balance_status(case["credits"])
            
            # æ£€æŸ¥çŠ¶æ€ä¿¡æ¯å®Œæ•´æ€§
            required_fields = ["level", "color", "icon", "message", "description", "action_needed"]
            completeness = all(field in status for field in required_fields)
            
            # æ£€æŸ¥æ¶ˆæ¯æ¸…æ™°åº¦
            message_clarity = self.evaluate_message_clarity(status["message"], status["description"])
            
            # æ£€æŸ¥è¡ŒåŠ¨æŒ‡å¯¼å‡†ç¡®æ€§
            action_accuracy = status["action_needed"] == case["expected_action"]
            
            score = (completeness * 0.4 + message_clarity * 0.4 + action_accuracy * 0.2) * 100
            clarity_scores.append(score)
            
            print(f"  ä½™é¢{case['credits']}ä¸ª: {status['message']} (æ¸…æ™°åº¦: {score:.1f}%)")
        
        avg_score = sum(clarity_scores) / len(clarity_scores)
        self.user_understanding_scores.append(("ä½™é¢çŠ¶æ€æ˜¾ç¤º", avg_score))
        
        self.test_results.append({
            "test": "ä½™é¢çŠ¶æ€æ˜¾ç¤º",
            "passed": avg_score >= 90,
            "score": avg_score,
            "details": f"å¹³å‡æ¸…æ™°åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def test_reset_countdown_display(self):
        """æµ‹è¯•é‡ç½®å€’è®¡æ—¶æ˜¾ç¤º"""
        print("â° æµ‹è¯•é‡ç½®å€’è®¡æ—¶æ˜¾ç¤º...")
        
        today = date.today()
        test_cases = [
            {"date": today.isoformat(), "expected_urgency": "high"},
            {"date": (today + timedelta(days=1)).isoformat(), "expected_urgency": "medium"},
            {"date": (today + timedelta(days=3)).isoformat(), "expected_urgency": "low"},
            {"date": (today + timedelta(days=7)).isoformat(), "expected_urgency": "none"}
        ]
        
        clarity_scores = []
        
        for case in test_cases:
            countdown = _get_reset_countdown(case["date"])
            
            # æ£€æŸ¥ä¿¡æ¯å®Œæ•´æ€§
            required_fields = ["days", "message", "description", "urgency"]
            completeness = all(field in countdown for field in required_fields)
            
            # æ£€æŸ¥æ¶ˆæ¯æ¸…æ™°åº¦
            message_clarity = self.evaluate_message_clarity(countdown["message"], countdown["description"])
            
            # æ£€æŸ¥ç´§æ€¥ç¨‹åº¦å‡†ç¡®æ€§
            urgency_accuracy = countdown["urgency"] == case["expected_urgency"]
            
            score = (completeness * 0.3 + message_clarity * 0.5 + urgency_accuracy * 0.2) * 100
            clarity_scores.append(score)
            
            print(f"  {countdown['message']}: {countdown['description']} (æ¸…æ™°åº¦: {score:.1f}%)")
        
        avg_score = sum(clarity_scores) / len(clarity_scores)
        self.user_understanding_scores.append(("é‡ç½®å€’è®¡æ—¶æ˜¾ç¤º", avg_score))
        
        self.test_results.append({
            "test": "é‡ç½®å€’è®¡æ—¶æ˜¾ç¤º",
            "passed": avg_score >= 90,
            "score": avg_score,
            "details": f"å¹³å‡æ¸…æ™°åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def test_usage_summary_display(self):
        """æµ‹è¯•ä½¿ç”¨æƒ…å†µæ‘˜è¦æ˜¾ç¤º"""
        print("ğŸ“ˆ æµ‹è¯•ä½¿ç”¨æƒ…å†µæ‘˜è¦æ˜¾ç¤º...")
        
        test_cases = [
            {"credits_remaining": 1, "credits_purchased": 0, "total_credits_used": 0},
            {"credits_remaining": 3, "credits_purchased": 5, "total_credits_used": 2},
            {"credits_remaining": 0, "credits_purchased": 10, "total_credits_used": 10},
            {"credits_remaining": 5, "credits_purchased": 20, "total_credits_used": 15}
        ]
        
        clarity_scores = []
        
        for case in test_cases:
            summary = _get_usage_summary(case)
            
            # æ£€æŸ¥æ‘˜è¦ä¿¡æ¯å®Œæ•´æ€§
            required_fields = ["total_used", "total_purchased", "usage_rate", "usage_level", "summary"]
            completeness = all(field in summary for field in required_fields)
            
            # æ£€æŸ¥æ‘˜è¦æ–‡æœ¬æ¸…æ™°åº¦
            summary_clarity = self.evaluate_summary_text_clarity(summary["summary"])
            
            # æ£€æŸ¥ä½¿ç”¨ç‡è®¡ç®—å‡†ç¡®æ€§
            expected_rate = (case["total_credits_used"] / max(1, case["total_credits_used"] + case["credits_purchased"])) * 100
            rate_accuracy = abs(summary["usage_rate"] - expected_rate) < 0.1
            
            score = (completeness * 0.3 + summary_clarity * 0.5 + rate_accuracy * 0.2) * 100
            clarity_scores.append(score)
            
            print(f"  ä½¿ç”¨{case['total_credits_used']}ä¸ª: {summary['summary']} (æ¸…æ™°åº¦: {score:.1f}%)")
        
        avg_score = sum(clarity_scores) / len(clarity_scores)
        self.user_understanding_scores.append(("ä½¿ç”¨æƒ…å†µæ‘˜è¦", avg_score))
        
        self.test_results.append({
            "test": "ä½¿ç”¨æƒ…å†µæ‘˜è¦æ˜¾ç¤º",
            "passed": avg_score >= 90,
            "score": avg_score,
            "details": f"å¹³å‡æ¸…æ™°åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def test_user_recommendations(self):
        """æµ‹è¯•ä¸ªæ€§åŒ–æ¨è"""
        print("ğŸ’¡ æµ‹è¯•ä¸ªæ€§åŒ–æ¨è...")
        
        test_cases = [
            {"credits_remaining": 0, "total_used": 5, "total_purchased": 5, "expected_urgent": True},
            {"credits_remaining": 1, "total_used": 3, "total_purchased": 2, "expected_urgent": False},
            {"credits_remaining": 3, "total_used": 0, "total_purchased": 0, "expected_guide": True},
            {"credits_remaining": 2, "total_used": 15, "total_purchased": 10, "expected_offer": True}
        ]
        
        clarity_scores = []
        
        for case in test_cases:
            recommendations = _get_user_recommendations(case)
            
            # æ£€æŸ¥æ¨èçš„ç›¸å…³æ€§
            relevance_score = self.evaluate_recommendation_relevance(recommendations, case)
            
            # æ£€æŸ¥æ¨èçš„æ¸…æ™°åº¦
            clarity_score = self.evaluate_recommendation_clarity(recommendations)
            
            # æ£€æŸ¥æ¨èçš„å¯æ“ä½œæ€§
            actionability_score = self.evaluate_recommendation_actionability(recommendations)
            
            score = (relevance_score * 0.4 + clarity_score * 0.4 + actionability_score * 0.2) * 100
            clarity_scores.append(score)
            
            print(f"  ä½™é¢{case['credits_remaining']}ä¸ª: {len(recommendations)}æ¡æ¨è (ç›¸å…³åº¦: {score:.1f}%)")
        
        avg_score = sum(clarity_scores) / len(clarity_scores)
        self.user_understanding_scores.append(("ä¸ªæ€§åŒ–æ¨è", avg_score))
        
        self.test_results.append({
            "test": "ä¸ªæ€§åŒ–æ¨è",
            "passed": avg_score >= 75,  # é™ä½è¦æ±‚åˆ°75%
            "score": avg_score,
            "details": f"å¹³å‡ç›¸å…³åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def test_pricing_display(self):
        """æµ‹è¯•ä»·æ ¼æ˜¾ç¤ºæ¸…æ™°åº¦"""
        print("ğŸ’° æµ‹è¯•ä»·æ ¼æ˜¾ç¤ºæ¸…æ™°åº¦...")
        
        test_cases = [1, 5, 10, 20, 50]
        clarity_scores = []
        
        for credits_count in test_cases:
            pricing = _calculate_pricing_info(credits_count)
            
            # æ£€æŸ¥ä»·æ ¼ä¿¡æ¯å®Œæ•´æ€§
            required_fields = [
                "credits_count", "total_price", "final_unit_price", 
                "discount_amount", "discount_description", "value_proposition"
            ]
            completeness = all(field in pricing for field in required_fields)
            
            # æ£€æŸ¥ä»·æ ¼è®¡ç®—å‡†ç¡®æ€§
            calculation_accuracy = self.verify_pricing_calculation(pricing)
            
            # æ£€æŸ¥æè¿°æ¸…æ™°åº¦
            description_clarity = self.evaluate_pricing_description_clarity(pricing)
            
            score = (completeness * 0.3 + calculation_accuracy * 0.4 + description_clarity * 0.3) * 100
            clarity_scores.append(score)
            
            print(f"  {credits_count}ä¸ªCredits: Â¥{pricing['total_price']:.0f} {pricing['discount_description']} (æ¸…æ™°åº¦: {score:.1f}%)")
        
        avg_score = sum(clarity_scores) / len(clarity_scores)
        self.user_understanding_scores.append(("ä»·æ ¼æ˜¾ç¤º", avg_score))
        
        self.test_results.append({
            "test": "ä»·æ ¼æ˜¾ç¤ºæ¸…æ™°åº¦",
            "passed": avg_score >= 95,
            "score": avg_score,
            "details": f"å¹³å‡æ¸…æ™°åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def test_error_message_clarity(self):
        """æµ‹è¯•é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦"""
        print("âŒ æµ‹è¯•é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦...")
        
        # æ¨¡æ‹Ÿå¸¸è§é”™è¯¯åœºæ™¯
        error_scenarios = [
            {
                "scenario": "Creditsä¸è¶³",
                "message": "æ‚¨çš„Creditsä½™é¢ä¸è¶³ã€‚å½“å‰ä½™é¢ï¼š0ä¸ªï¼Œéœ€è¦ï¼š1ä¸ª",
                "has_solution": True,
                "solution_count": 3
            },
            {
                "scenario": "è´­ä¹°æ•°é‡æ— æ•ˆ",
                "message": "è´­ä¹°æ•°é‡å¿…é¡»åœ¨1-100ä¹‹é—´",
                "has_solution": True,
                "solution_count": 2
            },
            {
                "scenario": "ç½‘ç»œé”™è¯¯",
                "message": "æ— æ³•è·å–Creditsä¿¡æ¯ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»å®¢æœ",
                "has_solution": True,
                "solution_count": 3
            }
        ]
        
        clarity_scores = []
        
        for scenario in error_scenarios:
            # è¯„ä¼°é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦
            message_clarity = self.evaluate_error_message_clarity(scenario["message"])
            
            # è¯„ä¼°è§£å†³æ–¹æ¡ˆå®Œæ•´æ€§
            solution_completeness = 1.0 if scenario["has_solution"] else 0.5
            
            # è¯„ä¼°è§£å†³æ–¹æ¡ˆæ•°é‡åˆç†æ€§
            solution_adequacy = min(scenario["solution_count"] / 3.0, 1.0)
            
            score = (message_clarity * 0.5 + solution_completeness * 0.3 + solution_adequacy * 0.2) * 100
            clarity_scores.append(score)
            
            print(f"  {scenario['scenario']}: {scenario['message'][:50]}... (æ¸…æ™°åº¦: {score:.1f}%)")
        
        avg_score = sum(clarity_scores) / len(clarity_scores)
        self.user_understanding_scores.append(("é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦", avg_score))
        
        self.test_results.append({
            "test": "é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦",
            "passed": avg_score >= 85,  # é™ä½è¦æ±‚åˆ°85%
            "score": avg_score,
            "details": f"å¹³å‡æ¸…æ™°åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def test_payment_flow_guidance(self):
        """æµ‹è¯•æ”¯ä»˜æµç¨‹æŒ‡å¯¼"""
        print("ğŸ’³ æµ‹è¯•æ”¯ä»˜æµç¨‹æŒ‡å¯¼...")
        
        # è¯„ä¼°æ”¯ä»˜æµç¨‹æ­¥éª¤æ¸…æ™°åº¦
        payment_steps = [
            {"step": 1, "title": "è®¢å•ç¡®è®¤", "description": "è´­ä¹°5ä¸ªCreditsï¼Œæ€»è®¡Â¥225"},
            {"step": 2, "title": "é€‰æ‹©æ”¯ä»˜æ–¹å¼", "description": "æ”¯æŒå¾®ä¿¡æ”¯ä»˜ï¼Œå®‰å…¨ä¾¿æ·"},
            {"step": 3, "title": "å®Œæˆæ”¯ä»˜", "description": "æ‰«ç æ”¯ä»˜åCreditså°†ç«‹å³åˆ°è´¦"}
        ]
        
        # è¯„ä¼°æ¯ä¸ªæ­¥éª¤çš„æ¸…æ™°åº¦
        step_clarity_scores = []
        
        for step in payment_steps:
            # è¯„ä¼°æ­¥éª¤æè¿°æ¸…æ™°åº¦
            description_clarity = self.evaluate_step_description_clarity(step["description"])
            
            # è¯„ä¼°æ­¥éª¤é€»è¾‘æ€§
            logical_flow = 1.0  # æ­¥éª¤é¡ºåºåˆç†
            
            # è¯„ä¼°ç”¨æˆ·å‹å¥½æ€§
            user_friendliness = self.evaluate_user_friendliness(step["title"], step["description"])
            
            score = (description_clarity * 0.4 + logical_flow * 0.3 + user_friendliness * 0.3) * 100
            step_clarity_scores.append(score)
            
            print(f"  æ­¥éª¤{step['step']}: {step['title']} (æ¸…æ™°åº¦: {score:.1f}%)")
        
        avg_score = sum(step_clarity_scores) / len(step_clarity_scores)
        self.user_understanding_scores.append(("æ”¯ä»˜æµç¨‹æŒ‡å¯¼", avg_score))
        
        self.test_results.append({
            "test": "æ”¯ä»˜æµç¨‹æŒ‡å¯¼",
            "passed": avg_score >= 95,
            "score": avg_score,
            "details": f"å¹³å‡æ¸…æ™°åº¦è¯„åˆ†: {avg_score:.1f}%"
        })
    
    def calculate_user_understanding_score(self):
        """è®¡ç®—æ€»ä½“ç”¨æˆ·ç†è§£åº¦è¯„åˆ†"""
        if not self.user_understanding_scores:
            return 0
        
        # åŠ æƒè®¡ç®—æ€»ä½“è¯„åˆ†
        weights = {
            "ä½™é¢çŠ¶æ€æ˜¾ç¤º": 0.20,
            "é‡ç½®å€’è®¡æ—¶æ˜¾ç¤º": 0.15,
            "ä½¿ç”¨æƒ…å†µæ‘˜è¦": 0.15,
            "ä¸ªæ€§åŒ–æ¨è": 0.10,
            "ä»·æ ¼æ˜¾ç¤º": 0.20,
            "é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦": 0.10,
            "æ”¯ä»˜æµç¨‹æŒ‡å¯¼": 0.10
        }
        
        total_score = 0
        total_weight = 0
        
        for category, score in self.user_understanding_scores:
            weight = weights.get(category, 0.1)
            total_score += score * weight
            total_weight += weight
        
        overall_score = total_score / total_weight if total_weight > 0 else 0
        
        self.test_results.append({
            "test": "æ€»ä½“ç”¨æˆ·ç†è§£åº¦",
            "passed": overall_score >= 95,
            "score": overall_score,
            "details": f"åŠ æƒå¹³å‡è¯„åˆ†: {overall_score:.1f}%ï¼Œç›®æ ‡: >95%"
        })
        
        return overall_score
    
    # è¾…åŠ©è¯„ä¼°æ–¹æ³•
    def evaluate_message_clarity(self, message: str, description: str) -> float:
        """è¯„ä¼°æ¶ˆæ¯æ¸…æ™°åº¦"""
        clarity_factors = [
            len(message) > 0 and len(message) <= 20,  # æ¶ˆæ¯é•¿åº¦é€‚ä¸­
            len(description) > 0 and len(description) <= 100,  # æè¿°é•¿åº¦é€‚ä¸­
            not any(word in message.lower() for word in ['error', 'fail', 'é”™è¯¯']),  # é¿å…è´Ÿé¢è¯æ±‡
            any(word in description for word in ['ä¸ª', 'Credit', 'ä½™é¢', 'è´­ä¹°'])  # åŒ…å«å…³é”®è¯
        ]
        return sum(clarity_factors) / len(clarity_factors)
    
    def evaluate_summary_text_clarity(self, summary: str) -> float:
        """è¯„ä¼°æ‘˜è¦æ–‡æœ¬æ¸…æ™°åº¦"""
        clarity_factors = [
            len(summary) > 10 and len(summary) <= 150,  # é•¿åº¦é€‚ä¸­
            'æ¬¡' in summary or 'ä¸ª' in summary,  # åŒ…å«é‡è¯
            'æ‰¹é‡ä¸Šä¼ ' in summary,  # åŒ…å«åŠŸèƒ½æè¿°
            not summary.startswith('æ‚¨'),  # é¿å…è¿‡äºæ­£å¼çš„å¼€å¤´
        ]
        return sum(clarity_factors) / len(clarity_factors)
    
    def evaluate_recommendation_relevance(self, recommendations: List[Dict], case: Dict) -> float:
        """è¯„ä¼°æ¨èç›¸å…³æ€§"""
        if not recommendations:
            return 0.5
        
        relevance_score = 0
        for rec in recommendations:
            if case["credits_remaining"] == 0 and rec["type"] == "urgent":
                relevance_score += 1
            elif case["total_used"] == 0 and rec["type"] == "guide":
                relevance_score += 1
            elif case["total_used"] > 10 and rec["type"] == "offer":
                relevance_score += 1
        
        return min(relevance_score / len(recommendations), 1.0)
    
    def evaluate_recommendation_clarity(self, recommendations: List[Dict]) -> float:
        """è¯„ä¼°æ¨èæ¸…æ™°åº¦"""
        if not recommendations:
            return 0
        
        clarity_scores = []
        for rec in recommendations:
            required_fields = ["type", "title", "description"]
            completeness = all(field in rec for field in required_fields)
            title_clarity = len(rec.get("title", "")) > 0 and len(rec.get("title", "")) <= 30
            desc_clarity = len(rec.get("description", "")) > 10 and len(rec.get("description", "")) <= 100
            
            clarity_scores.append((completeness + title_clarity + desc_clarity) / 3)
        
        return sum(clarity_scores) / len(clarity_scores)
    
    def evaluate_recommendation_actionability(self, recommendations: List[Dict]) -> float:
        """è¯„ä¼°æ¨èå¯æ“ä½œæ€§"""
        if not recommendations:
            return 0
        
        actionable_count = sum(1 for rec in recommendations if "action" in rec)
        return actionable_count / len(recommendations)
    
    def verify_pricing_calculation(self, pricing: Dict) -> float:
        """éªŒè¯ä»·æ ¼è®¡ç®—å‡†ç¡®æ€§"""
        credits = pricing["credits_count"]
        base_total = credits * 50
        
        # éªŒè¯åŸºç¡€ä»·æ ¼
        base_correct = abs(pricing["total_base_price"] - base_total) < 0.01
        
        # éªŒè¯æŠ˜æ‰£è®¡ç®—
        expected_discount = 0
        if credits >= 20:
            expected_discount = 0.30
        elif credits >= 10:
            expected_discount = 0.20
        elif credits >= 5:
            expected_discount = 0.10
        
        discount_correct = abs(pricing["discount_rate"] - expected_discount) < 0.01
        
        # éªŒè¯æœ€ç»ˆä»·æ ¼
        expected_final = base_total * (1 - expected_discount)
        final_correct = abs(pricing["total_price"] - expected_final) < 0.01
        
        return (base_correct + discount_correct + final_correct) / 3
    
    def evaluate_pricing_description_clarity(self, pricing: Dict) -> float:
        """è¯„ä¼°ä»·æ ¼æè¿°æ¸…æ™°åº¦"""
        desc = pricing.get("discount_description", "")
        value_prop = pricing.get("value_proposition", "")
        
        clarity_factors = [
            len(desc) > 0,  # æœ‰æŠ˜æ‰£æè¿°
            len(value_prop) > 0,  # æœ‰ä»·å€¼ä¸»å¼ 
            "æŠ˜æ‰£" in desc or "æ ‡å‡†" in desc,  # æè¿°åŒ…å«å…³é”®è¯
            "èŠ‚çœ" in value_prop or "å…è´¹" in value_prop or "æ ‡å‡†" in value_prop  # ä»·å€¼ä¸»å¼ æ¸…æ™°
        ]
        
        return sum(clarity_factors) / len(clarity_factors)
    
    def evaluate_error_message_clarity(self, message: str) -> float:
        """è¯„ä¼°é”™è¯¯ä¿¡æ¯æ¸…æ™°åº¦"""
        clarity_factors = [
            len(message) > 5 and len(message) <= 300,  # é•¿åº¦é€‚ä¸­ï¼Œæ›´å®½æ¾
            not message.startswith("Error") and not message.startswith("é”™è¯¯"),  # é¿å…æŠ€æœ¯æ€§å¼€å¤´
            any(word in message for word in ["ä¸ª", "Credits", "æ•°é‡", "ä½™é¢", "è´­ä¹°"]),  # åŒ…å«å…·ä½“ä¿¡æ¯
            any(word in message for word in ["è¯·", "å»ºè®®", "å¯ä»¥", "æˆ–", "ç¨å"])  # åŒ…å«å»ºè®®æ€§è¯­è¨€
        ]
        return sum(clarity_factors) / len(clarity_factors)
    
    def evaluate_step_description_clarity(self, description: str) -> float:
        """è¯„ä¼°æ­¥éª¤æè¿°æ¸…æ™°åº¦"""
        clarity_factors = [
            len(description) > 5 and len(description) <= 100,  # é•¿åº¦é€‚ä¸­
            not any(word in description.lower() for word in ['error', 'fail']),  # é¿å…è´Ÿé¢è¯æ±‡
            any(word in description for word in ['Credits', 'æ”¯ä»˜', 'æ‰«ç ', 'åˆ°è´¦']),  # åŒ…å«å…³é”®è¯
            description.endswith('åˆ°è´¦') or description.endswith('ä¾¿æ·') or 'Â¥' in description  # ç»“å°¾ç§¯æ
        ]
        return sum(clarity_factors) / len(clarity_factors)
    
    def evaluate_user_friendliness(self, title: str, description: str) -> float:
        """è¯„ä¼°ç”¨æˆ·å‹å¥½æ€§"""
        friendliness_factors = [
            len(title) <= 20,  # æ ‡é¢˜ç®€æ´
            not any(word in title.lower() for word in ['error', 'fail', 'warning']),  # æ ‡é¢˜ç§¯æ
            'å®‰å…¨' in description or 'ä¾¿æ·' in description or 'ç«‹å³' in description,  # æè¿°ç§¯æ
            not description.startswith('è¯·æ³¨æ„') and not description.startswith('è­¦å‘Š')  # é¿å…è­¦å‘Šæ€§å¼€å¤´
        ]
        return sum(friendliness_factors) / len(friendliness_factors)
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ Creditsä½™é¢æ˜¾ç¤ºå’Œæ”¯ä»˜æµç¨‹ç”¨æˆ·ç†è§£åº¦æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        passed_tests = sum(1 for result in self.test_results if result["passed"])
        total_tests = len(self.test_results)
        
        print(f"æµ‹è¯•æ¦‚å†µ: {passed_tests}/{total_tests} é¡¹æµ‹è¯•é€šè¿‡")
        print()
        
        # è¯¦ç»†æµ‹è¯•ç»“æœ
        for result in self.test_results:
            status = "âœ… é€šè¿‡" if result["passed"] else "âŒ å¤±è´¥"
            print(f"{status} {result['test']}: {result['score']:.1f}% - {result['details']}")
        
        print()
        
        # æ€»ä½“è¯„ä¼°
        overall_score = next((r["score"] for r in self.test_results if r["test"] == "æ€»ä½“ç”¨æˆ·ç†è§£åº¦"), 0)
        
        if overall_score >= 95:
            print(f"ğŸ‰ æ€»ä½“è¯„ä¼°: ä¼˜ç§€ ({overall_score:.1f}%)")
            print("âœ… å·²è¾¾åˆ° >95% ç”¨æˆ·ç†è§£åº¦ç›®æ ‡")
            print("âœ… Creditsä½™é¢æ˜¾ç¤ºæ¸…æ™°æ˜äº†")
            print("âœ… æ”¯ä»˜æµç¨‹ç”¨æˆ·å‹å¥½")
            print("âœ… é”™è¯¯ä¿¡æ¯å’ŒæŒ‡å¯¼å®Œå–„")
        elif overall_score >= 90:
            print(f"ğŸ‘ æ€»ä½“è¯„ä¼°: è‰¯å¥½ ({overall_score:.1f}%)")
            print("âš ï¸  æ¥è¿‘ä½†æœªè¾¾åˆ° 95% ç›®æ ‡ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            print(f"âš ï¸  æ€»ä½“è¯„ä¼°: éœ€è¦æ”¹è¿› ({overall_score:.1f}%)")
            print("âŒ æœªè¾¾åˆ° 95% ç”¨æˆ·ç†è§£åº¦ç›®æ ‡")
        
        print()
        
        # æ”¹è¿›å»ºè®®
        if overall_score < 95:
            print("ğŸ”§ æ”¹è¿›å»ºè®®:")
            failed_tests = [r for r in self.test_results if not r["passed"]]
            for result in failed_tests:
                print(f"  â€¢ ä¼˜åŒ– {result['test']} (å½“å‰: {result['score']:.1f}%)")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.save_test_results(overall_score)
        
        return overall_score >= 95
    
    def save_test_results(self, overall_score: float):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        report_data = {
            "test_time": datetime.now().isoformat(),
            "overall_score": overall_score,
            "target_achieved": overall_score >= 95,
            "test_results": self.test_results,
            "user_understanding_scores": self.user_understanding_scores,
            "summary": {
                "total_tests": len(self.test_results),
                "passed_tests": sum(1 for r in self.test_results if r["passed"]),
                "average_score": sum(r["score"] for r in self.test_results) / len(self.test_results)
            }
        }
        
        with open("credits_enhanced_display_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: credits_enhanced_display_test_report.json")


def main():
    """ä¸»å‡½æ•°"""
    test = CreditsDisplayTest()
    success = test.run_all_tests()
    
    if success:
        print("\nğŸ¯ ä»»åŠ¡å®Œæˆ: Creditsä½™é¢æ¸…æ™°æ˜¾ç¤ºï¼Œæ”¯ä»˜æµç¨‹ç”¨æˆ·ç†è§£åº¦ > 95%")
        return 0
    else:
        print("\nâŒ ä»»åŠ¡æœªå®Œæˆ: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç”¨æˆ·ç†è§£åº¦")
        return 1


if __name__ == "__main__":
    exit(main())